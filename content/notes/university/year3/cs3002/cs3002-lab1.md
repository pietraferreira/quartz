---
title:  "Lab 1 - Unsupervised Learning"
tags:
  - university
module: ai
lecturer: Allan Tucker
created: 2023-10-04
year: '3'
type: lab
---
---
Relates to the [[notes/university/year3/cs3002/cs3002-unsupervised-learning|unsupervised learning]] lecture.

**Q1)** Given the following three attributes:

| ID             | 1   | 2   | 3   | 4   | 5   | 6   | 7   | 8   | 9   | 10  |
| -------------- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Age            | 23  | 57  | 18  | 32  | 26  | 71  | 66  | 43  | 52  | 18  |
| Blood Pressure | 95  | 150 | 85  | 110 | 100 | 80  | 100 | 105 | 95  | 90  |
| BMI            | 25  | 20  | 27  | 23  | 18  | 30  | 27  | 25  | 35  | 29    |

Choose any 2 patients and calculate:

I chose 3 and 4.

**a)** What is the Euclidean distance between each of them?

ed = $\sqrt{(32-18)^2+(110-85)^2+(23-27)^2} =$

= $\sqrt{196 + 625 + 16}$

= $\sqrt{837} = 29$

**b)** What is the Manhattan distance between each of them?

$|32 - 18| + |110 - 85| + |23-27| =$

= $14 + 25 + 4 = 43$

**Q2)** Briefly describe three different forms of hierarchical clustering methods.

- Single linkage: smallest distance between any two pairs
- Average linkage: average distance between pairs
- Complete linkage: largest distance between any two pairs

**Q3)** Describe two clustering methods and their advantages/disadvantages.

- **Hierarchical**: it is a way of grouping things together based on how similar or different they are from each other. For example, if you have different animals, you start by putting each in its own group. Then, we look at them and put the most similar together in a new group. You keep doing this until you have a few big groups that represent different categories of animals.  We merge close clusters together and the result is a dendrogram.

Pros:
- can produce an ordering of the objects, could be informative for data display.
- smaller clusters, being helpful for discovery.

 Cons:
 - cant reallocate object that has been 'incorrectly' grouped at an early stage.
 - different distance metrics for measuring distances might generate different results.
  
- **K-Means**: we first decide how many groups we want, for example 3. we then randomly pick 3 "items" to be the centre of each group. now, we look at each item and see which centre it is closest to, we put then with the closest centre. after we have them all together in groups, we find a new centre by taking the average of all the items in the group. we keep repeating this until the centres don't change too much.

Pros:
- can be computationally faster than hierarchical if K is small.
- may produce tighter clusters than hierarchical.

Cons:
- fixed number of clusters can make it difficult to figure out K.
- different initial partitions can result in different final clusters.
- does not work well with non-globular clusters.



